---
title: "HW 2"
author: "James Topor"
date: "June 18, 2016"
output: html_document
---

```{r}
library(knitr)

```

1. Download the classification output data set (attached in Blackboard to the assignment).

```{r}
hw2 <- read.csv("https://raw.githubusercontent.com/jtopor/CUNY-MSDA-621/master/HW-2/classification-output-data.csv", stringsAsFactors = FALSE)  
```

2. Use the table() function to get the raw confusion matrix for this scored dataset. Make sure you understand the output. In particular, do the rows represent the actual or predicted class? The columns?

```{r}
# extract the three relevant columns
hw2.d <- hw2[,c(9,10,11)]

######################################################
# extracted columns MUST be converted to matrix format

# actual = class
actual <- as.matrix(hw2.d[,1])

# predicted = scored.class
predicted <- as.matrix(hw2.d[,2])

#########################################
# rows = actual
# cols = predicted

# left column = NEGATIVES
# right column = POSITIVES
#########################################


# upper left = True Negative  | Upper right = False Positive
# lower left = False Negative | lower right = True Positive


table(actual, predicted)

# display an interpretable version of the matrix
tmp <- data.frame(table(actual, predicted))
tmp
```

The rows are the actual class.

The columns are the predicted class.

# ------------------------------------------------------
 
3. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the accuracy of the predictions.
```{r}
accuracy <- function(actual, predicted){
  
  # Equation to be modeled: (TP + TN) / (TP + FP + TN + FN)
  
  # derive confusion matrix cell values
  c.mat <- data.frame(table(actual, predicted))
  
  # extract all four confusion matrix values from the data frame
  TN <- as.numeric(as.character(c.mat[1,3]))
  FN <- as.numeric(as.character(c.mat[2,3]))
  FP <- as.numeric(as.character(c.mat[3,3]))
  TP <- as.numeric(as.character(c.mat[4,3]))
  
  # now calculate the required metric
  return( (TP + TN) / (TP + FP + TN + FN) )
}
```

4. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the classification error rate of the predictions.

```{r}
classif.err.rate <- function(actual, predicted) {
  
  # Equation to be modeled: (FP + FN) / (TP + FP + TN + FN)
  
  # derive confusion matrix cell values
  c.mat <- data.frame(table(actual, predicted))
  
  # extract all four confusion matrix values from the data frame
  TN <- as.numeric(as.character(c.mat[1,3]))
  FN <- as.numeric(as.character(c.mat[2,3]))
  FP <- as.numeric(as.character(c.mat[3,3]))
  TP <- as.numeric(as.character(c.mat[4,3]))
  
  # now calculate the required metric
  return( (FP + FN) / (TP + FP + TN + FN) )
}  
```

Now Verify that you get an accuracy and an error rate that sums to one:

```{r}
(acc.1 <- accuracy(actual, predicted) )
(cer.1 <- classif.err.rate(actual, predicted) )

acc.1 + cer.1
```

It does sum to one

# -------------------

5. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the precision of the predictions.

```{r}
precision <- function(actual, predicted) {
  
  # Equation to be modeled: TP / (TP + FP)
  
  # derive confusion matrix cell values
  c.mat <- data.frame(table(actual, predicted))
  
  # extract all four confusion matrix values from the data frame
  TN <- as.numeric(as.character(c.mat[1,3]))
  FN <- as.numeric(as.character(c.mat[2,3]))
  FP <- as.numeric(as.character(c.mat[3,3]))
  TP <- as.numeric(as.character(c.mat[4,3]))
  
  # now calculate the required metric
  return( TP / (TP + FP) )
}  
```

6. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the sensitivity of the predictions. Sensitivity is also known as recall.

```{r}
sensitivity <- function(actual, predicted) {
  
  # Equation to be modeled: TP / (TP + FN)
  
  # derive confusion matrix cell values
  c.mat <- data.frame(table(actual, predicted))
  
  # extract all four confusion matrix values from the data frame
  TN <- as.numeric(as.character(c.mat[1,3]))
  FN <- as.numeric(as.character(c.mat[2,3]))
  FP <- as.numeric(as.character(c.mat[3,3]))
  TP <- as.numeric(as.character(c.mat[4,3]))
  
  # now calculate the required metric
  return( TP / (TP + FN) )
}  
```


7. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the specificity of the predictions.

```{r}
specificity <- function(actual, predicted) {
  
  # Equation to be modeled: TN / (TN + FP)
  
  # derive confusion matrix cell values
  c.mat <- data.frame(table(actual, predicted))
  
  # extract all four confusion matrix values from the data frame
  TN <- as.numeric(as.character(c.mat[1,3]))
  FN <- as.numeric(as.character(c.mat[2,3]))
  FP <- as.numeric(as.character(c.mat[3,3]))
  TP <- as.numeric(as.character(c.mat[4,3]))
  
  # now calculate the required metric
  return( TN / (TN + FP) )
}  
```


8. Write a function that takes the data set as a dataframe, with actual and predicted classifications identified, and returns the F1 score of the predictions.

```{r}
F1.Score <- function(actual, predicted) {
  
  # Equation to be modeled: ( 2 * precision * sensitivity) / (precision + sensitivity)
  
  # now calculate the required metric
  return( ( 2 * precision(actual, predicted) * sensitivity(actual, predicted)) / (precision(actual, predicted) + sensitivity(actual, predicted)) )
}  
```

Now test F1.score
```{r}
F1.Score(actual, predicted)
```